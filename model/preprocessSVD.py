import pickle
from keras_preprocessing.sequence import pad_sequences
from keras.models import load_model
import tensorflow as tf
import pandas as pd
class PreprocessSVD:
    def __init__(self):
        save_option = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')

        self.tokenizer = pickle.load(open("./vuln_tokenizer.sav", 'rb'))
        self.cnn_model = load_model("./vuln_model", options=save_option)
        self.keywords = set(open("./keywords.csv", "r").read().split("\n"))
    
    def clean_code(self, code_part):
        newregex = r"\b(?!alignas|class|alignof|not_eq|ifdef|co_yield|protected|noexcept|const_cast|namespace|error|consteval|thread_local|bool|throw|co_return|synchronized|module|explicit|mutable|and_eq|operator|co_await|for|catch|__has_include|pragma|or|continue|dynamic_cast|or_eq|signed|union|ifndef|xor_eq|void|else|goto|this|typedef|switch|virtual|char16_t|sizeof|concept|line|typename|requires|static_cast|try|false|private|default|char|while|break|asm|constinit|warning|true|unsigned|compl|transaction_safe_dynamic|define|override|friend|include|int|delete|atomic_cancel|using|reflexpr|bitand|xor|elifdef|enum|const|wchar_t|decltype|static|nullptr|not|reinterpret_cast|endif|atomic_commit|public|and|inline|long|static_assert|new|template|undef|char32_t|final|case|do|volatile|constexpr|register|atomic_noexcept|extern|bitor|export|float|char8_t|import|struct|defined|typeid|short|return|if|__has_cpp_attribute|double|transaction_safe|elif|elifndef|auto)\w+\b"    
        regex_for_removing_single_line_comments = r"(//.*)|(/\*.*\*/)"
        code_part = code_part.replace(regex_for_removing_single_line_comments, "", regex=True)
        code_part = code_part.replace(r'\r+|\n+|\t+|  ','', regex=True)
        regex_for_removing_variables = r"\b(?!" 
        regex_for_removing_variables += r"|".join(list(self.keywords))
        regex_for_removing_variables += r")\w+\b"
        subst = "variable"
        code_part = code_part.replace(newregex, subst, regex=True)    
        # Regex for removing comments
        regex_for_removing_multi_line_comments = r"(//.*)|(/\*.*\*/)"
        code_part = code_part.replace(regex_for_removing_multi_line_comments, "", regex=True)
        return code_part



    def getVuln(self, code_part, threshold):
        l = []
        # since clean_code accepts df
        code_part = pd.DataFrame([code_part])
        code_part = self.clean_code(code_part)[0].iloc[0]
        print(code_part)
        l.append(code_part)
        self.tokenizer.fit_on_texts(l)
        sequences = self.tokenizer.texts_to_sequences(l)
        x_inp = pad_sequences(sequences, maxlen=500)
        pred = self.cnn_model.predict(x_inp)
        for i in range(5):
            if pred[0][i] < threshold:
                pred[0][i] = 0
            else:
                pred[0][i] = 1
        return pred.tolist()
